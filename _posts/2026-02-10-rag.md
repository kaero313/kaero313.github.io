---
title: "[RAG] 최신 AI 서비스들이 '답변 출처'를 표시할 수 있는 이유"
date: 2026-02-10 16:32:27 +0900
categories: [AI, RAG]
tags: [AI, RAG, LLM]
preview_image: assets/img/rag/prev_icon.png
---

RPA를 주로 하는 레거시, 폐쇄 환경 위주의 기업에서도 AI의 바람은 피할 수 없는지, 요즘 회사에서 RAG를 이용하여 학습을 하고 주기적으로 내부 POC를 진행하자는 얘기가 나왔다.<br/>
진행하면서 겪은 내용 및 문제점 등 모든 경험을 정리해 볼 예정이다.
<br/>
<br/>

<div align="center">
  <img src="assets/img/rag/rag-logo.png" alt="RAG 아이콘" width="500">
</div>
> 이것마저 AI(나노 바나나)로 생성한 이미지이다..

<br/>

# RAG란?
---

RAG(Retrieval-Augmented Generation, 검색 증강 생성)를 한 문장으로 요약하면 'AI가 모르는 내용을 외부 데이터베이스에서 직접 찾아보고 답변하게 만드는 기술'이라고 AI가 답변을 해줬다.

## LLM의 한계

AGI의 영역에 도달하여 LLM의 로직을 새로 구성하지 않는 이상 LLM의 한계는 명확하다.<br/>
- 새롭게 학습이 되기 전까지는 최신 데이터가 없다.
- 회사 내부 데이터같은 대중적이지 않은 데이터는 없다.
- 비싸다.

등등... 이러한 문제점을 해결하기 위하여 RAG라는 신개념의 기술이 나왔다고 한다.

## RAG 아키텍쳐

<div align="center">
  <img src="assets/img/rag/work_flow.png" alt="RAG 워크 플로우" width="500">
</div>

결론적으로 최종 목표는 LLM에 보다 더 잘 질문하려고 사용하는 것이다. <br/>
DB에 필요한 데이터를 넣어두고, 사용자의 질문에 따라 필요한 데이터를 꺼내서 기존의 프롬포트와 합쳐 LLM에 질문하면 끝이다.<br/>

<br/>

# RAG 구현 
---

처음엔 회사에서 진행을 했었고, 그 후에 따로 개인적으로도 구현을 했었다. 

- 회사에서 하던 방식
  - LangFlow 기반으로 만들어진 솔루션을 이용하여 기존 시스템과의 연동 기반
- 개인적으로 하던 방식
  - FastAPI를 이용하여 백앤드 위주에 채팅용 UI 구성

  